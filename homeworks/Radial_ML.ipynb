{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721a60c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes # Разрешено для загрузки датасета\n",
    "from sklearn.model_selection import train_test_split # Разрешено для разделения данных\n",
    "from sklearn.metrics import mean_squared_error # Разрешено для оценки\n",
    "\n",
    "# Импортируем нашу реализацию RBF Kernel Regression из предыдущего блока\n",
    "# Предполагается, что класс SimpleRBFKernelRegression определен выше или в другом месте\n",
    "# В рамках этого блока мы его повторно определим для самодостаточности.\n",
    "\n",
    "class SimpleRBFKernelRegression:\n",
    "    \"\"\"\n",
    "    Простая реализация Radial basis kernel regression\n",
    "    с использованием Гауссовского (RBF) ядра.\n",
    "    Использует только numpy и pandas.\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma=1.0, lambda_reg=0.0):\n",
    "        \"\"\"\n",
    "        Инициализация Radial basis kernel regression.\n",
    "\n",
    "        Args:\n",
    "            sigma (float): Параметр ширины Гауссовского ядра. Контролирует радиус влияния\n",
    "                           каждой обучающей точки. По умолчанию 1.0.\n",
    "            lambda_reg (float): Параметр регуляризации (L2). Добавляет небольшую\n",
    "                                регуляризацию для устойчивости. По умолчанию 0.0.\n",
    "        \"\"\"\n",
    "        self.sigma = sigma\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self._trained = False\n",
    "\n",
    "    def _rbf_kernel(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Гауссовское (RBF) ядро.\n",
    "\n",
    "        Args:\n",
    "            x1 (np.ndarray): Первая точка(и).\n",
    "            x2 (np.ndarray): Вторая точка(и).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Значение(я) ядра.\n",
    "        \"\"\"\n",
    "        # Вычисляем squared Euclidean distance: ||x1 - x2||^2\n",
    "        # Для двух векторов (1D): sum((x1 - x2)**2)\n",
    "        # Для массива точек X1 и одной точки x2: sum((X1 - x2)**2, axis=1)\n",
    "        # Для двух массивов точек X1 и X2 (матрица ядра):\n",
    "        # K_ij = exp(-gamma * ||x_i - x_j||^2), gamma = 1 / (2 * sigma^2)\n",
    "        # ||x_i - x_j||^2 = ||x_i||^2 + ||x_j||^2 - 2 * x_i.T * x_j\n",
    "\n",
    "        # Убедимся, что x1 и x2 - numpy массивы\n",
    "        x1 = np.asarray(x1)\n",
    "        x2 = np.asarray(x2)\n",
    "\n",
    "        # Вычисляем гамма\n",
    "        gamma = 1.0 / (2 * self.sigma**2)\n",
    "\n",
    "        # Если x1 и x2 - одномерные векторы (одна точка каждый)\n",
    "        if x1.ndim == 1 and x2.ndim == 1:\n",
    "            sq_dist = np.sum((x1 - x2)**2)\n",
    "            return np.exp(-gamma * sq_dist)\n",
    "        # Если x1 - массив точек, x2 - одна точка\n",
    "        elif x1.ndim > 1 and x2.ndim == 1:\n",
    "             # Используем broadcasting для вычисления разницы и квадрата\n",
    "             sq_dist = np.sum((x1 - x2)**2, axis=1)\n",
    "             return np.exp(-gamma * sq_dist)\n",
    "        # Если x1 - одна точка, x2 - массив точек\n",
    "        elif x1.ndim == 1 and x2.ndim > 1:\n",
    "             sq_dist = np.sum((x1 - x2)**2, axis=1)\n",
    "             return np.exp(-gamma * sq_dist)\n",
    "        # Если x1 и x2 - массивы точек (вычисляем полную матрицу ядра)\n",
    "        elif x1.ndim > 1 and x2.ndim > 1:\n",
    "             # Более эффективный способ вычисления матрицы расстояний\n",
    "             # ||x_i - x_j||^2 = ||x_i||^2 + ||x_j||^2 - 2 * x_i.T * x_j\n",
    "             # sum(X1^2, axis=1)[:, None] - столбец сумм квадратов X1\n",
    "             # sum(X2^2, axis=1)[None, :] - строка сумм квадратов X2\n",
    "             # X1 @ X2.T - матрица скалярных произведений\n",
    "             sq_dists = np.sum(x1**2, axis=1)[:, None] + np.sum(x2**2, axis=1)[None, :] - 2 * (x1 @ x2.T)\n",
    "             # Избегаем отрицательных значений из-за ошибок округления\n",
    "             sq_dists[sq_dists < 0] = 0\n",
    "             return np.exp(-gamma * sq_dists)\n",
    "        else:\n",
    "             raise ValueError(\"Неподдержимые размерности для ядра\")\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Обучает модель Radial basis kernel regression.\n",
    "        В этом методе мы просто сохраняем обучающие данные.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray или pd.DataFrame): Входные признаки обучающих данных.\n",
    "            y (np.ndarray или pd.Series): Целевая переменная обучающих данных.\n",
    "        \"\"\"\n",
    "        self.X_train = np.asarray(X)\n",
    "        self.y_train = np.asarray(y)\n",
    "        self._trained = True\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Прогнозирует значения для новых данных.\n",
    "\n",
    "        Args:\n",
    "            X_test (np.ndarray или pd.DataFrame): Входные признаки новых данных.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Спрогнозированные значения.\n",
    "        \"\"\"\n",
    "        if not self._trained:\n",
    "            raise RuntimeError(\"Модель не обучена. Вызовите fit сначала.\")\n",
    "\n",
    "        X_test = np.asarray(X_test)\n",
    "        n_test_samples = X_test.shape[0]\n",
    "        n_train_samples = self.X_train.shape[0]\n",
    "\n",
    "        if n_train_samples == 0:\n",
    "             return np.zeros(n_test_samples, dtype=float)\n",
    "\n",
    "        # Вычисляем матрицу ядра между тестовыми и обучающими данными\n",
    "        # K_test_train[i, j] = kernel(X_test[i], X_train[j])\n",
    "        K_test_train = self._rbf_kernel(X_test, self.X_train)\n",
    "\n",
    "        # Вычисляем матрицу ядра между обучающими данными (для регуляризации)\n",
    "        K_train_train = self._rbf_kernel(self.X_train, self.X_train)\n",
    "\n",
    "        # Добавляем регуляризацию к диагонали матрицы ядра обучающих данных\n",
    "        # (K_train_train + lambda_reg * I)\n",
    "        K_train_train_reg = K_train_train + self.lambda_reg * np.eye(n_train_samples)\n",
    "\n",
    "        # Решаем систему линейных уравнений для нахождения весов (альфа)\n",
    "        # (K_train_train + lambda_reg * I) * alpha = y_train\n",
    "        # alpha = inv(K_train_train + lambda_reg * I) * y_train\n",
    "        try:\n",
    "            alpha = np.linalg.solve(K_train_train_reg, self.y_train)\n",
    "        except np.linalg.LinAlgError:\n",
    "            # Если матрица вырожденная, используем псевдоинверсию (менее устойчиво)\n",
    "            print(\"Предупреждение: Матрица ядра плохо обусловлена. Использование псевдоинверсии.\")\n",
    "            alpha = np.linalg.pinv(K_train_train_reg) @ self.y_train\n",
    "\n",
    "\n",
    "        # Вычисляем предсказания\n",
    "        # y_pred = K_test_train @ alpha\n",
    "        predictions = K_test_train @ alpha\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "# 1. Загрузка реального датасета (Diabetes)\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "\n",
    "# 2. Разделение данных на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Размер обучающей выборки: {X_train.shape[0]} примеров\")\n",
    "print(f\"Размер тестовой выборки: {X_test.shape[0]} примеров\")\n",
    "print(f\"Количество признаков: {X_train.shape[1]}\")\n",
    "\n",
    "\n",
    "# 3. Применение нашей реализации RBF Kernel Regression\n",
    "# Подбираем гиперпараметры (sigma и lambda_reg) - это важный шаг в реальных задачах.\n",
    "# Здесь используем фиксированные значения для примера.\n",
    "# В реальной задаче нужно использовать кросс-валидацию для подбора.\n",
    "my_rbfkr = SimpleRBFKernelRegression(sigma=0.5, lambda_reg=0.1)\n",
    "\n",
    "# Обучение модели\n",
    "my_rbfkr.fit(X_train, y_train)\n",
    "\n",
    "# Предсказание на тестовой выборке\n",
    "my_predictions = my_rbfkr.predict(X_test)\n",
    "\n",
    "# 4. Оценка производительности на тестовой выборке\n",
    "my_mse_test = mean_squared_error(y_test, my_predictions)\n",
    "print(f\"\\nMSE нашей реализации RBF Kernel Regression на тестовой выборке: {my_mse_test:.4f}\")\n",
    "\n",
    "# Опционально: Сравнение с sklearn SVR с RBF ядром\n",
    "# try:\n",
    "#     from sklearn.svm import SVR\n",
    "#\n",
    "#     # Подбираем гиперпараметры для SVR (C, gamma)\n",
    "#     # gamma для SVR аналогичен 1 / (2 * sigma^2)\n",
    "#     sklearn_svr = SVR(kernel='rbf', C=100.0, gamma=1.0/(2*my_rbfkr.sigma**2))\n",
    "#     sklearn_svr.fit(X_train, y_train)\n",
    "#     sklearn_predictions = sklearn_svr.predict(X_test)\n",
    "#     sklearn_mse_test = mean_squared_error(y_test, sklearn_predictions)\n",
    "#     print(f\"MSE sklearn SVR (RBF) на тестовой выборке: {sklearn_mse_test:.4f}\")\n",
    "#\n",
    "# except ImportError:\n",
    "#     print(\"\\nБиблиотека sklearn не найдена для сравнения SVR.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"\\nОшибка при сравнении со sklearn SVR: {e}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

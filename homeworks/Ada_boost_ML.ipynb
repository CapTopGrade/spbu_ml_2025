{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dac8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Вспомогательный класс: Решающий пень (Decision Stump)\n",
    "class DecisionStump:\n",
    "    \"\"\"\n",
    "    Простой решающий пень для бинарной классификации.\n",
    "    Классифицирует на основе порогового значения одного признака.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.feature_index = None # Индекс признака, по которому происходит разделение\n",
    "        self.threshold = None     # Пороговое значение для разделения\n",
    "        self.polarity = 1         # Направление неравенства (1 или -1)\n",
    "        self.alpha = None         # Вес этого пня в финальном ансамбле\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Прогнозирует метки классов для входных данных.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray или pd.DataFrame): Входные данные.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Спрогнозированные метки классов (-1 или 1).\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        n_samples = X.shape[0]\n",
    "        predictions = np.ones(n_samples)\n",
    "\n",
    "        # Применяем правило пня: если значение признака <= порога, то метка polarity, иначе -polarity\n",
    "        if self.polarity == 1:\n",
    "            predictions[X[:, self.feature_index] <= self.threshold] = -1\n",
    "        else: # polarity == -1\n",
    "            predictions[X[:, self.feature_index] > self.threshold] = -1\n",
    "\n",
    "        return predictions\n",
    "\n",
    "# Реализация алгоритма AdaBoost\n",
    "class SimpleAdaBoost:\n",
    "    \"\"\"\n",
    "    Простая реализация алгоритма AdaBoost для бинарной классификации\n",
    "    с использованием решающих пней в качестве слабых классификаторов.\n",
    "    Использует только numpy и pandas.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators=50):\n",
    "        \"\"\"\n",
    "        Инициализация AdaBoost.\n",
    "\n",
    "        Args:\n",
    "            n_estimators (int): Количество слабых классификаторов (решающих пней). По умолчанию 50.\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.estimators = [] # Список слабых классификаторов (DecisionStump)\n",
    "        self.estimator_weights = [] # Веса каждого классификатора (alpha)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Обучает модель AdaBoost на данных.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray или pd.DataFrame): Входные признаки.\n",
    "            y (np.ndarray или pd.Series): Метки классов (-1 или 1).\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Проверяем, что метки классов -1 или 1\n",
    "        if not np.all(np.isin(y, [-1, 1])):\n",
    "             raise ValueError(\"Метки классов должны быть -1 или 1\")\n",
    "\n",
    "        # Инициализируем веса примеров данных равномерно\n",
    "        sample_weights = np.full(n_samples, (1 / n_samples))\n",
    "\n",
    "        self.estimators = []\n",
    "        self.estimator_weights = []\n",
    "\n",
    "        # Основной цикл AdaBoost\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Находим лучший решающий пень для текущих весов примеров\n",
    "            stump = self._find_best_stump(X, y, sample_weights)\n",
    "\n",
    "            # Вычисляем ошибку классификации пня\n",
    "            predictions = stump.predict(X)\n",
    "            misclassified = (predictions != y)\n",
    "            # Взвешенная ошибка: сумма весов неправильно классифицированных примеров\n",
    "            error = np.sum(sample_weights[misclassified])\n",
    "\n",
    "            # Избегаем деления на ноль или логарифмирования нуля\n",
    "            if error > 0.5 or error < 1e-9:\n",
    "                 # Если ошибка > 0.5, пень хуже случайного угадывания. Останавливаемся.\n",
    "                 # Если ошибка очень близка к 0, пень идеален. Присваиваем ему большой вес.\n",
    "                 alpha = 1000.0 if error < 1e-9 else 0.0\n",
    "            else:\n",
    "                # Вычисляем вес пня (alpha)\n",
    "                alpha = 0.5 * np.log((1.0 - error) / (error + 1e-10)) # Добавляем эпсилон для стабильности\n",
    "\n",
    "            # Обновляем веса примеров\n",
    "            # Увеличиваем веса неправильно классифицированных примеров\n",
    "            # Уменьшаем веса правильно классифицированных примеров\n",
    "            # new_weight = old_weight * exp(-alpha * y * prediction)\n",
    "            sample_weights *= np.exp(-alpha * y * predictions)\n",
    "\n",
    "            # Нормализуем веса примеров, чтобы их сумма была равна 1\n",
    "            sample_weights /= np.sum(sample_weights)\n",
    "\n",
    "            # Сохраняем пень и его вес\n",
    "            stump.alpha = alpha\n",
    "            self.estimators.append(stump)\n",
    "            self.estimator_weights.append(alpha)\n",
    "\n",
    "    def _find_best_stump(self, X, y, sample_weights):\n",
    "        \"\"\"\n",
    "        Находит лучший решающий пень, минимизирующий взвешенную ошибку.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Входные признаки.\n",
    "            y (np.ndarray): Метки классов (-1 или 1).\n",
    "            sample_weights (np.ndarray): Веса каждого примера.\n",
    "\n",
    "        Returns:\n",
    "            DecisionStump: Лучший найденный решающий пень.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        best_stump = None\n",
    "        min_error = float('inf')\n",
    "\n",
    "        # Перебираем все признаки\n",
    "        for feature_index in range(n_features):\n",
    "            # Получаем уникальные значения признака, чтобы использовать их как потенциальные пороги\n",
    "            feature_values = X[:, feature_index]\n",
    "            unique_values = np.unique(feature_values)\n",
    "\n",
    "            # Перебираем все уникальные значения как потенциальные пороги\n",
    "            # Пороги обычно выбирают между уникальными значениями\n",
    "            thresholds = (unique_values[:-1] + unique_values[1:]) / 2 if len(unique_values) > 1 else unique_values\n",
    "\n",
    "            if len(thresholds) == 0:\n",
    "                 # Если только одно уникальное значение, используем его как порог\n",
    "                 thresholds = unique_values\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                # Проверяем два варианта полярности (направления неравенства)\n",
    "                for polarity in [1, -1]:\n",
    "                    stump = DecisionStump()\n",
    "                    stump.feature_index = feature_index\n",
    "                    stump.threshold = threshold\n",
    "                    stump.polarity = polarity\n",
    "\n",
    "                    # Вычисляем ошибку для текущего пня\n",
    "                    predictions = stump.predict(X)\n",
    "                    misclassified = (predictions != y)\n",
    "                    error = np.sum(sample_weights[misclassified])\n",
    "\n",
    "                    # Если текущая ошибка меньше минимальной, обновляем лучший пень\n",
    "                    if error < min_error:\n",
    "                        min_error = error\n",
    "                        best_stump = stump\n",
    "\n",
    "        return best_stump\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Прогнозирует метки классов для входных данных, используя ансамбль пней.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray или pd.DataFrame): Входные данные.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Спрогнозированные метки классов (-1 или 1).\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        n_samples = X.shape[0]\n",
    "        # Сумма взвешенных прогнозов от всех пней\n",
    "        weighted_predictions = np.zeros(n_samples)\n",
    "\n",
    "        for stump, alpha in zip(self.estimators, self.estimator_weights):\n",
    "            weighted_predictions += alpha * stump.predict(X)\n",
    "\n",
    "        # Финальное решение: знак суммы взвешенных прогнозов\n",
    "        # Используем signbit для корректной обработки нуля\n",
    "        predictions = np.where(np.signbit(weighted_predictions), -1, 1)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "# 2. Симуляция данных для тестирования AdaBoost\n",
    "\n",
    "# Генерируем простые двумерные данные для бинарной классификации\n",
    "np.random.seed(42)\n",
    "n_samples_sim = 100\n",
    "\n",
    "# Класс -1: точки вокруг (-1, -1)\n",
    "X_neg = np.random.multivariate_normal([-1, -1], [[0.5, 0], [0, 0.5]], n_samples_sim // 2)\n",
    "y_neg = np.full(n_samples_sim // 2, -1)\n",
    "\n",
    "# Класс 1: точки вокруг (1, 1)\n",
    "X_pos = np.random.multivariate_normal([1, 1], [[0.5, 0], [0, 0.5]], n_samples_sim // 2)\n",
    "y_pos = np.full(n_samples_sim // 2, 1)\n",
    "\n",
    "X_sim = np.vstack((X_neg, X_pos))\n",
    "y_sim = np.hstack((y_neg, y_pos))\n",
    "\n",
    "# Перемешиваем данные\n",
    "shuffle_indices = np.random.permutation(n_samples_sim)\n",
    "X_sim = X_sim[shuffle_indices]\n",
    "y_sim = y_sim[shuffle_indices]\n",
    "\n",
    "# 3. Тестирование и сравнение\n",
    "\n",
    "# Используем нашу реализацию AdaBoost\n",
    "my_adaboost = SimpleAdaBoost(n_estimators=10) # Используем 10 пней для примера\n",
    "my_adaboost.fit(X_sim, y_sim)\n",
    "my_predictions = my_adaboost.predict(X_sim)\n",
    "\n",
    "# Вычисляем точность нашей модели\n",
    "my_accuracy = np.mean(my_predictions == y_sim)\n",
    "print(f\"Точность нашей реализации AdaBoost на симулированных данных: {my_accuracy:.4f}\")\n",
    "\n",
    "# Сравнение со стандартной реализацией (sklearn)\n",
    "# Для сравнения разрешено использовать sklearn\n",
    "try:\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    # Используем AdaBoostClassifier из sklearn с решающими пнями (max_depth=1)\n",
    "    # random_state для воспроизводимости\n",
    "    sklearn_adaboost = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1),\n",
    "                                          n_estimators=10, random_state=42, algorithm='SAMME') # SAMME для дискретных прогнозов\n",
    "    sklearn_adaboost.fit(X_sim, y_sim)\n",
    "    sklearn_predictions = sklearn_adaboost.predict(X_sim)\n",
    "\n",
    "    # Вычисляем точность sklearn\n",
    "    sklearn_accuracy = accuracy_score(y_sim, sklearn_predictions)\n",
    "    print(f\"Точность sklearn AdaBoost на симулированных данных: {sklearn_accuracy:.4f}\")\n",
    "\n",
    "    # Сравнение предсказаний (могут немного отличаться из-за деталей реализации)\n",
    "    agreement = np.mean(my_predictions == sklearn_predictions)\n",
    "    print(f\"Доля совпадений предсказаний нашей и sklearn реализаций: {agreement:.4f}\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\nБиблиотека sklearn не найдена. Пропуск сравнения.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nОшибка при сравнении со sklearn: {e}\")\n",
    "\n",
    "# Визуализация границы решений (опционально, требует matplotlib)\n",
    "# import matplotlib.pyplot as plt\n",
    "#\n",
    "# def plot_decision_boundary(predict_func, X, y, title):\n",
    "#     x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "#     y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "#     xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "#                          np.arange(y_min, y_max, 0.1))\n",
    "#\n",
    "#     Z = predict_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "#     Z = Z.reshape(xx.shape)\n",
    "#\n",
    "#     plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "#     plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k')\n",
    "#     plt.title(title)\n",
    "#     plt.xlabel('Признак 1')\n",
    "#     plt.ylabel('Признак 2')\n",
    "#\n",
    "# plt.figure(figsize=(12, 5))\n",
    "#\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plot_decision_boundary(my_adaboost.predict, X_sim, y_sim, 'Наша реализация AdaBoost')\n",
    "#\n",
    "# if 'sklearn_adaboost' in locals():\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plot_decision_boundary(sklearn_adaboost.predict, X_sim, y_sim, 'sklearn AdaBoost')\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
